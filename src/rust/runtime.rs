//! The Rust runtime for Menhir
//!
//! All the parsers generated by [Menhir] with the `--rust` option (or using
//! the [`menhir`] package from crates.io) use functions and types from this
//! crate.
//!
//! This documentation is a reference API of the runtime library. It might be
//! useful to learn how to interact with the genarated parser from Rust code.
//! To learn how to write grammars, see the [Menhir manual]. To learn how to use
//! the generator to produce an executable Rust parser, see
//! [the documentation of the `menhir` crate].
//!
//! [Menhir]: http://gallium.inria.fr/~fpottier/menhir/
//! [`menhir`]: https://crates.io/crates/menhir
//! [Menhir manual]: http://gallium.inria.fr/~fpottier/menhir/manual.pdf
//!
//! # The runtime API
//!
//! The runtime library in this crate implements a classic LR(1) automaton loop
//! to interpret the parse tables generated from a grammar specification. This
//! implementation is highly generic in order to hide implementation details in
//! the generated code. The most notable elements are:
//!
//! * [`Lexer`], the lexer interface ; see also [`IteratorLexer`].
//! * [`LRParser`], the trait implemented by the generated parser, that exposes
//!   functions to access the generated parse table.
//!
//! [`Lexer`]: trait.Lexer.html
//! [`IteratorLexer`]: struct.IteratorLexer.html
//! [`LRParser`]: trait.LRParser.html
//!
//! # The lexer interface
//!
//! Menhir parsers do not require a specially-recognized EOF token. The user is
//! free to declare such a token in its specification but there is nothing
//! special about it: Menhir parsers are able to automatically detect when they
//! successfuly parsed recognized a start non-terminal without comsuming any
//! extra lookahead token. This way, what remains of the input can be used with
//! another invokation of the parser, or anything else.
//!
//! To represent this fact, the runtime uses its own [`Lexer`] trait, which
//! encodes an infinite stream of pairs of a token and its location.
//!
//! ```rust
//! trait Lexer {
//!     type Location;
//!     type Token;
//!     type Error;
//!
//!     fn input(&mut self) -> Result<(Self::Location, Self::Token),
//!                                   Self::Error>;
//! }
//! ```

use self::internals::*;
use self::lexing::*;

/// The internal representation of the LR engine.
///
/// Types and functions exposed in this module have to pe public because they
/// are used from the generated code but should not be considered stable for the
/// moment.
pub mod internals {
    pub type Stack<YYType, State> = Vec<(State, YYType)>;

    // The type of semantic actions.
    // Some(ptr) = a flat (not closure) code pointer to the handler
    // None = this is a start reduction. it's actually never executed but
    // indicates an Accept action instead. Since NULL is not a valid function
    // pointer in correct Rust code, this should be optimized to be just the size
    // of a function pointer.
    pub type SemAct<YYType, State> = fn(State, &mut Stack<YYType, State>)
                                        -> State;

    // An action, i.e. an entry in the action table. Error, accept, reduce with a
    // semantic function that takes the stack, modifie it and returns the next state
    // or shift to state, discarding the current token.
    pub enum Action<YYType, State> {
        Err,
        Reduce(Option<SemAct<YYType, State>>),
        Shift(State)
    }

    impl<T, U: Copy> Copy for Action<T, U> {}

    impl<T, U: Clone> Clone for Action<T, U> {
        fn clone(&self) -> Self {
            match *self {
                Action::Err => Action::Err,
                Action::Reduce(act) => Action::Reduce(act),
                Action::Shift(ref shift) => Action::Shift(shift.clone())
            }
        }
    }
}

/// The lexer interface.
pub mod lexing {
    /// The lexer interface.
    ///
    /// This trait describes the interface that a Menhir parser expects from the
    /// lexer. It can be seen as an infinite iterator over tokens and locations,
    /// that can also report errors.
    ///
    /// Several lexer generators or lexing tools expose a simpler interface based
    /// on the [`Iterator`] trait. They can be easily converted to this interface
    /// using [`IteratorLexer`].
    /// 
    /// [`Iterator`]: https://doc.rust-lang.org/stable/std/iter/trait.Iterator.html
    /// [`IteratorLexer`]: struct.IteratorLexer.html
    ///
    /// The stream of tokens
    pub trait Lexer {
        /// A type that describes the position of a token in the input source,
        /// for example a line and column number.
        ///
        /// The actual value of this type is left purely at the discretion of the
        /// user. Menhir does not make any assumption on it and simply carries it
        /// through the parsing process to report it when an error occurs. The
        /// user can use any type appropriate for their error reporting
        /// mechanism, or even `()` if tracking of locations is not desired.
        type Location;

        /// The type of tokens returned by this lexer.
        type Token;

        /// A type that describes the possible errors that the lexer might
        /// encounter while processing the input source.
        ///
        /// As with `Location`, Menhir does not make any assumption on this type
        /// and simply returns it unaltered when the lexer returns it. The user
        /// can use any type appropriate for their error reporting mechanism, or
        /// even `()` if if this lexer cannot fail.
        type Error;

        /// Reads the next token from the input.
        ///
        /// On success, returns a pair of the token and its location in the input
        /// source. On failure, returns a value describing the error. Returning
        /// an error from the lexer will cause a Menhir parser to stop the
        /// parsing process and report it.
        ///
        /// Menhir parsers are able to detect successful termination without
        /// requiring extra lookahead. Because of this, no special value is
        /// required to indicate the end of the stream. If the end of the stream
        /// is encountered, `input` should report an error. This behaviour allows
        /// the parser not to consume any token it does not need and leave the
        /// lexer intact to be used for another parsing process. If one does want
        /// the parser to use exactly the whole input, one should either
        /// implement explicitly an “EOF” token in the grammar, or simply
        /// manually check that the lexer has no more tokens to offer once
        /// parsing succeeded.
        fn input(&mut self) -> Result<(Self::Location, Self::Token), Self::Error>;
    }

    /// Adapter type to convert an iterator into a lexer.
    ///
    /// Lexing tools that implement an interface based on [`Iterator`] can be
    /// converted to Menhir's more advanced [`Lexer`] interface by using this
    /// type.
    ///
    /// If the iterator returns `None` while the parser still needs tokens,
    /// `input` will return an [`UnexpectedEof`] error. If one wants the parser
    /// to consume exactly all the tokens, one should manually check that the
    /// underlying iterator is empty once parsing succeeded.
    ///
    /// [`Iterator`]: https://doc.rust-lang.org/stable/std/iter/trait.Iterator.html
    /// [`Lexer`]: trait.Lexer.html
    /// [`UnexpectedEof`]: struct.UnexpectedEof.html
    pub struct IteratorLexer<Iter, Loc, Tok>
        where Iter: Iterator<Item = (Loc, Tok)> {
        iter: Iter,
        last_pos: Loc,
        marker: ::std::marker::PhantomData<(Loc, Tok)>
    }

    /// This error indicates that the lexer reached the end of the input stream
    /// while the parser was still expecting input.
    ///
    /// It is used when using an [`IteratorLexer`] as input, if the underlying
    /// iterator returns `None`. It can also be used to report the end of the
    /// stream when implementing a custom [`Lexer`].
    ///
    /// [`IteratorLexer`]: struct.IteratorLexer.html
    /// [`Lexer`]: trait.Lexer.html
    #[derive(Clone, Copy, Debug)]
    pub struct UnexpectedEof<Location>(pub Location);

    impl<Iter, Loc, Tok> Lexer for IteratorLexer<Iter, Loc, Tok>
        where Loc: Clone, Iter: Iterator<Item = (Loc, Tok)> {
        type Location = Loc;
        type Token = Tok;
        type Error = UnexpectedEof<Self::Location>;

        fn input(&mut self) -> Result<(Loc, Tok), Self::Error> {
            match self.iter.next() {
                Some((pos, tok)) => {
                    self.last_pos = pos.clone();
                    Ok((pos, tok))
                }
                None => Err(UnexpectedEof(self.last_pos.clone()))
            }
        }
    }

    impl<Iter, Loc, Tok> IteratorLexer<Iter, Loc, Tok>
        where Loc: Default, Iter: Iterator<Item = (Loc, Tok)> {
        /// Builds a new adapter from the given iterator.
        ///
        /// This function takes the iterator by-value. If the iterator is not,
        /// `Copy`, it won't be usable anymore after the parsing process. If this
        /// is not desired, one should give to this function a mutable reference
        /// to the iterator instead.
        pub fn new(lex: Iter) -> Self {
            IteratorLexer {
                iter: lex,
                last_pos: Loc::default(),
                marker: ::std::marker::PhantomData
            }
        }
    }
}

// Parser interface.

/// Trait describing a Menhir parser.
///
/// This trait is implemented by the parsers generated by Menhir and describes
/// the parser interface. Menhir should generate a single type implenting this
/// trait for each grammar file.
///
/// The user of the generated parser should probably not use this interface
/// directly and see [`EntryPoint`] instead. This interface should not be
/// considered stable.
///
/// [`EntryPoint`]: trait.EntryPoint.html
pub trait LRParser {
    type Terminal: Copy;
    fn error() -> Self::Terminal;

    type State: Copy;
    type YYType;

    fn default_reduction(state: Self::State)
                         -> Option<Option<SemAct<Self::YYType, Self::State>>>;
    fn action(state: Self::State, token: Self::Terminal)
              -> Action<Self::YYType, Self::State>;
}

/// The trait of the entry points of a grammar file.
///
/// This trait describes the main interface of a Menhir parser. Menhir will
/// generate one type implementing this trait for each possible entry point of
/// the grammar file.
pub trait EntryPoint<Parser: LRParser> {
    /// The type of the semantic value produced by a successful parsing.
    type Output;

    /// Extracts the semantic value produced by a succesful parsing from the
    /// resulting stack.
    ///
    /// Calling this function on a stack which is not the result of a successful
    /// parse will most likely panic, but might also return an unspecified but
    /// valid value.
    fn extract_output(stack: Stack<Parser::YYType, Parser::State>) -> Self::Output;

    /// Returns the initial state of the parser automaton for this entry point.
    fn initial() -> Parser::State;

    /// Creates a new parser for this entry point from the given lexer.
    ///
    /// This function is similar to `run` but instead of running the parsing
    /// process, it does nothing and returns a [`ParserState`] instead that
    /// exposes the current state of the parser. However, the interface of
    /// [`ParserState`] should not be considered stable for the moment.
    ///
    /// [`ParserState`]: struct.ParserState.html
    fn new<Lexer>() -> ParserState<Lexer, Parser, Self>
        where Lexer: self::Lexer,
              Lexer::Location: Clone,
              Lexer::Token: Into<(Parser::YYType, Parser::Terminal)>,
              Self: ::std::marker::Sized {
        ParserState::<Lexer, Parser, Self>::new()
    }

    /// Runs the parsing process for this entry point from the given lexer.
    ///
    /// This requires that the tokens returned by the lexer can be converted
    /// into the `Terminal` type of the parser, which is the internal
    /// representation of tokens. Menhir should have generated an
    /// implementation of `Into` for the good token type.
    ///
    /// If a syntax error is encountered in the input stream, a Menhir parser
    /// will try to recover from it by applying the default strategy which
    /// consists in unwinding the stack until finding a state which accepts a
    /// shift or reduce action on the special `error` token. If the bottom of
    /// the stack is reached, a syntax error is returned. See the relevant
    /// section of the [manual] for more details.
    ///
    /// This function will parse input until one of the following events:
    /// * The parser successfully recognized the start production, in which
    ///   case it will return a value of `Ok(v)`, where `v` is the semantic
    ///   value.
    /// * The parser encountered an unrecoverable error, such as a lexer error
    ///   or a syntax error which it failed to recover from, in which case it
    ///   will return `Err(e)`. See [`Error`] for more details.
    ///
    /// [manual]: http://gallium.inria.fr/~fpottier/menhir/manual.pdf
    /// [`Error`]: enum.Error.html
    fn run<Lexer>(lex: Lexer) -> Result<Self::Output, Error<Lexer, Parser>>
        where Lexer: self::Lexer,
              Lexer::Token: Into<(Parser::YYType, Parser::Terminal)>,
              Lexer::Location: Clone,
              Parser: LRParser,
              Self: ::std::marker::Sized {
        let state = Self::new();
        state.run(lex)
    }
}

/// A fatal (non-recoverable parsing error).
///
/// This is the type returned by a Menhir parser upon encountering an error it
/// cannot or failed to recover from.
pub enum Error<Lexer: self::Lexer, Parser: LRParser> {
    /// The parser encountered a syntax error that couldn't be recovered.
    /// See [`SyntaxError`].
    ///
    /// [`SyntaxError`]: struct.SyntaxError.html
    SyntaxError(SyntaxError<Lexer, Parser>),

    /// The lexer returned an error when asked for a token, typically an IO
    /// error or the end of the stream. The carried value depends on the actual
    /// instance of [`Lexer`] being used.
    ///
    /// [`Lexer`]: lexing/trait.Lexer.html
    LexerError(Lexer::Error)
}

/// An unrecoverable syntax error.
///
/// Allows to extract the position in the input stream and the automaton state
/// in which the error was detected.
pub struct SyntaxError<Lexer: self::Lexer, Parser: LRParser> {
    loc: Lexer::Location,
    state: Parser::State
}

/// Extension trait for parser that can provide detailed error messages.
///
/// Menhir will generate an implementation of this trait for the parser type if
/// the grammar file was compiled with the `--compile-errors` option. See the
/// [manual] for information about how to use this feature.
///
/// [manual]: http://gallium.inria.fr/~fpottier/menhir/manual.pdf
pub trait LRErrors: LRParser {
    /// Gives a proper error message to describe the situation in which the
    /// parser entered error-handling mode.
    ///
    /// Returns the message provided by the user for this state in the error
    /// file given to the `--compiler-errors` option.
    fn message(state: Self::State) -> Option<&'static str>;
}

impl<Parser: LRErrors, Lexer: self::Lexer> SyntaxError<Lexer, Parser> {
    /// Returns a propre error message to describe this error, if such a
    /// message was specified.
    ///
    /// This require the parser to implement [`LRErrors`], which is the case
    /// when the grammar is compiled with the `--compile-error` option. See the
    /// [manual] for information about how to use this feature.
    ///
    /// [`LRErrors`]: trait.LRErrors.html
    /// [manual]: http://gallium.inria.fr/~fpottier/menhir/manual.pdf
    pub fn as_str(&self) -> Option<&'static str> {
        Parser::message(self.state)
    }
}

impl<Parser: LRParser, Lexer: self::Lexer> SyntaxError<Lexer, Parser> {
    /// Returns the location in the input stream of the token where the error
    /// was detected.
    ///
    /// The actual type of this value depends on the instance of [`Lexer`] being
    /// used.
    ///
    /// [`Lexer`]: lexing/trait.Lexer.html
    pub fn location(&self) -> &Lexer::Location {
        &self.loc
    }
}

// Incremental interface.

/// The state of the parser during the parsing process.
///
/// The interface of this type should not be considered stable.
/// In the future, this type will be used to expose the incremental interface,
/// which allows to run the parsing process step-by-step and access its
/// internals
pub struct ParserState<Lexer, Parser, Entry>
    where Parser: LRParser,
          Lexer: self::Lexer,
          Entry: EntryPoint<Parser> {
    state: Parser::State,
    yylval: Parser::YYType,
    lookahead: Parser::Terminal,
    location: Lexer::Location,
    stack: Stack<Parser::YYType, Parser::State>,
    entry: ::std::marker::PhantomData<Entry>,
    error: Option<(Parser::State, Lexer::Location)>
}

enum Checkpoint<Lexer, Parser, Entry>
    where Parser: LRParser,
          Lexer: self::Lexer,
          Entry: EntryPoint<Parser> {
    InputNeeded(ParserState<Lexer, Parser, Entry>),
    Reducing(ParserState<Lexer, Parser, Entry>,
             SemAct<Parser::YYType, Parser::State>,
             bool),
    Shifting(ParserState<Lexer, Parser, Entry>, Parser::State),
    HandlingError(ParserState<Lexer, Parser, Entry>),
    Accepted(Entry::Output),
    Rejected(SyntaxError<Lexer, Parser>)
}

// Several functions that each run a single step of the parsing process.
// They contain the actual logic of the LR loop.
// This interface is unsafe for now because some functions will leave the parser
// in an invalid state and cause undefined behaviour if they are not called in
// the right order.
// For this reason, they are not exposed but only used to implement the standard
// monolithic interface.
// We will make the incremental interface public once we add some kind of
// (preferably static) safety checks to those functions.
impl<Lexer, Parser, Entry> ParserState<Lexer, Parser, Entry>
    where Parser: LRParser,
          Lexer: self::Lexer,
          Lexer::Location: Clone,
          Lexer::Token: Into<(Parser::YYType, Parser::Terminal)>,
          Entry: EntryPoint<Parser> {

    fn new() -> ParserState<Lexer, Parser, Entry> {
        use std::mem::uninitialized;
        unsafe { ParserState {
            state: Entry::initial(),
            stack: Vec::new(),
            yylval: uninitialized(),
            lookahead: uninitialized(),
            location: uninitialized(),
            entry: ::std::marker::PhantomData,
            error: None
        }}
    }

    // Start the parsing process: we need lookahead. Stop and ask for input.
    fn start(self) -> Checkpoint<Lexer, Parser, Entry> {
        self.check_default_reductions(true)
    }

    // Called by the user to provide us input.
    fn offer(self, (loc, tok): (Lexer::Location, Lexer::Token))
             -> Checkpoint<Lexer, Parser, Entry> {
        self.discard(loc, tok)
    }

    // We have new input, discard the current lookahead token.
    fn discard(mut self, loc: Lexer::Location, tok: Lexer::Token)
               -> Checkpoint<Lexer, Parser, Entry> {
        let (yylval, tok) = tok.into();
        // Here yylval cannot contain a valid object, don't call dtor.
        unsafe {
            let val = ::std::mem::replace(&mut self.yylval, yylval);
            ::std::mem::forget(val);
        }
        self.lookahead = tok;
        self.location = loc;
        self.error = None;
        self.check_error()
    }

    // Before performing any action, check if there are default reductions.
    fn check_default_reductions(self, discard: bool)
                                -> Checkpoint<Lexer, Parser, Entry> {
        if let Some(red) = Parser::default_reduction(self.state) {
            self.announce_reduce(red, discard)
        } else if discard {
            Checkpoint::InputNeeded(self)
        } else {
            self.check_error()
        }
    }

    // If self error is not None, then we are currently in the process of
    // handling an error (i.e. the current lookahead token is the error token).
    // If this is the case, stop and expose an error handling checkpoint to the
    // user to ask them what to do.
    // Otherwise, perform an action.
    fn check_error(self) -> Checkpoint<Lexer, Parser, Entry> {
        if let Some(_) = self.error {
            Checkpoint::HandlingError(self)
        } else {
            // action
            match Parser::action(self.state, self.lookahead) {
                Action::Shift(shift) => self.shift(shift),
                Action::Reduce(reduce) => self.announce_reduce(reduce, false),
                Action::Err => self.initiate_error()
            }
        }
    }

    // We have a shift action. Perform it, push the current semantic value, and
    // expose it to the user. After that checkpoint, we should go back to start()
    // to get new input.
    fn shift(mut self, shift: Parser::State)
             -> Checkpoint<Lexer, Parser, Entry> {
        self.stack.push((self.state, self.yylval));
        // Here yylval has been moved so this is ok.
        self.yylval = unsafe { ::std::mem::uninitialized() };
        self.state = shift;
        Checkpoint::Shifting(self, shift)
    }

    // We have a reduce action. If this was the start production, accept the
    // input. Otherwise, expose a reduction checkpoint to the user.
    fn announce_reduce(self, red: Option<SemAct<Parser::YYType, Parser::State>>,
                       discard: bool) -> Checkpoint<Lexer, Parser, Entry> {
        match red {
            Some(code) => Checkpoint::Reducing(self, code, discard),
            None => self.accept()
        }
    }

    // Callend by the user to perform the reduction checkpoint.
    // Call the semantic action and go back directly to checking for default
    // reductions, since the current state changed but not the lookahead token.
    fn reduce(mut self, code: SemAct<Parser::YYType, Parser::State>,
              discard: bool) -> Checkpoint<Lexer, Parser, Entry> {
        self.state = code(self.state, &mut self.stack);
        self.check_default_reductions(discard)
    }

    // Accept the input and return the final semantic value.
    fn accept(self) -> Checkpoint<Lexer, Parser, Entry> {
        Checkpoint::Accepted(Entry::extract_output(self.stack))
    }

    // We do not have any action, so the current token is errorneous.
    // Begin error handling mode, remember the current state and position to
    // generate an error message later.
    // Then stop and expose the current situation to the user.
    fn initiate_error(mut self) -> Checkpoint<Lexer, Parser, Entry> {
        self.error = Some((self.state, self.location.clone()));
        Checkpoint::HandlingError(self)
    }

    // Called by the user to perform a step of error handling.
    // This is actually an action table lookup with the error token.
    fn error_step(self) -> Checkpoint<Lexer, Parser, Entry> {
        match Parser::action(self.state, Parser::error()) {
            // we have a shift on the error terminal
            Action::Shift(shift) => self.shift(shift),
            // not sure it's even possible to have an accepting reduction
            // on error but let's leave it at least for type safety...
            Action::Reduce(None) => self.accept(),
            // don't expose reduction steps taken during error handling
            Action::Reduce(Some(reduce)) => self.reduce(reduce, false),
            Action::Err => self.error_continue()
        }
    }

    // No action possible on the error token in this state, unwind a stack cell
    // and expose to the user that we are still in error handling mode.
    fn error_continue(mut self) -> Checkpoint<Lexer, Parser, Entry> {
        return match self.stack.pop() {
            Some((state, _)) => {
                self.state = state;
                Checkpoint::HandlingError(self)
            }

            None => {
                // No more stack cells, abort.
                match self.error {
                    Some((state, loc)) =>
                        Checkpoint::Rejected(SyntaxError {
                            state: state,
                            loc: loc
                        }),
                    None => unreachable!()
                }
            }
        }
    }
}

// Implements the monolithic interface in terms of the incremental interface,
// with the default error handling strategy.
impl<Lexer, Parser, Entry> ParserState<Lexer, Parser, Entry>
    where Parser: LRParser,
          Lexer: self::Lexer,
          Lexer::Token: Into<(Parser::YYType, Parser::Terminal)>,
          Lexer::Location: Clone,
          Entry: EntryPoint<Parser> {
    /// Run the parsing process until success or failure.
    ///
    /// This function actually implements the [`EntryPoint::run`] function in
    /// terms of the internal, unstable incremental interface. See
    /// [`EntryPoint::run`] for more details about the behaviour of this
    /// function.
    ///
    /// [`EntryPoint::run`]: trait.EntryPoint.html
    pub fn run(self, mut lexer: Lexer)
               -> Result<Entry::Output, Error<Lexer, Parser>> {
        let mut checkpoint = self.start();
        loop {
            checkpoint = match checkpoint {
                Checkpoint::Rejected(err)        =>
                    return Err(Error::SyntaxError(err)),
                Checkpoint::Accepted(out)        => return Ok(out),
                Checkpoint::HandlingError(state) => state.error_step(),
                Checkpoint::Shifting(state, _)   => state.start(),
                Checkpoint::Reducing(state, act, discard) => state.reduce(act, discard),
                Checkpoint::InputNeeded(state)   => {
                    match lexer.input() {
                        Ok(input) => state.offer(input),
                        Err(err) => return Err(Error::LexerError(err))
                    }
                }
            }
        }
    }
}
